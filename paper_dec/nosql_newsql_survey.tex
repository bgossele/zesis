\documentclass{IEEEtran}
\usepackage{cite}
\usepackage{url}
\usepackage{lscape}
\usepackage[final]{pdfpages}
\usepackage{graphicx}
\usepackage{pdflscape}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{array}

\begin{document}

\title{A survey on NoSQL and NewSQL datastores}
\author{Brecht Gossel\'e}
\maketitle

\begin{abstract}
Advances in bioinformatics have driven down the cost of genome sequencing dramatically over the past years. This raises the question how to efficiently and flexibly handle and store the large amount of data this process generates. Similar developments in web technology have spawned numerous NoSQL and NewSQL datastores in the last decade, which offer robust, highly scalable distributed storage and flexible data modelling. This paper reviews 6 such datastores and compares them on characteristics relevant to high performance computing in general and bioinformatics more specifically.
\end{abstract}


\section{Introduction}

Scientific progress has caused the cost of genome sequencing to drop at an exponential rate over the past decade and a half, even outpacing Moore's law since 2008 \cite{wetterstrand_sequencing_cost}. Because in all sorts of biological, medical and pharmaceutical research, more and more genomes are being sequenced, the amount of generated data increases rapidly. For instance, the whole genome sequencing pipeline of the Broad Institute\cite{broad_institute}, a reference in the field, generates in the order of 3 TB of intermediary data when sequencing a single human genome. Also, when sequenced with 50x mean coverage (denoting the average number of times a base has been read during the process\cite{coverage_definition}) a single human genome takes 50 GB to store in compressed format\footnote{This information was gathered during conversations with researchers at the Exascience Life Lab}, so as this scales to genomes of millions of people and other organisms, storage and processing requirements become ever more demanding in terms of scalability, latency and concurrency.
\\A logical evolution has been to tackle these problems with high performance computing systems. The Exascience Life Lab of imec, Intel, Janssen Pharmaceutica and 5 Flemish universities, actively researches the application of supercomputers for accelerating the processing of whole genome sequences \cite{exascience_life_lab}\cite{lifelab_BWA}.
\\Increasing popularity of web services such as social networks has caused a similar explosion of data for web companies. To handle this so-called Big Data\cite{mashey1997big} in an adequate way, traditional relational DBMS no longer suffice. Therefore, large web companies such as Google and Amazon have developped new storage solutions that meet the demands for high and incremental scalability, low latency and high availability\cite{baker2011megastore}. This has spawned many so called NoSQL ('Not only SQL') databases, which loosen the rigid relational datamodel in favour of better scaling and easier distribution of the data.
\\NoSQL datastores come in multiple flavours and can be divided in a few cate\-gories based on the datamodel they use:
\begin{itemize}
\item Key-value stores: much like dictionaries and associative maps, these map unique keys to values. Values are uninterpreted byte arrays, and the only way to access them is by their key. This means modeling relations and complex structures between data, rich querying and indexing is not possible\cite{hecht2011nosql}\cite{grolinger2013data}. 
\item Columnar stores: based on the datamodel pioneered by Google's BigTable, these store data in "a sparse, distributed, persistent multidimensional sorted map" \cite{chang2008bigtable}. In BigTable's case this is a map of row key, column key and a timestamp. In this way, multiple versions of the data can be stored in chronological order. Because the system doesn't interpret the data, relationships can't be modeled. This is left to application logic\cite{hecht2011nosql}.
\item Document stores: these store data as key-value pairs encapsulated in documents. Values can be of a wide variety of types, such as nested documents, lists or scalar values. Attribute names can be dynamically specified at runtime and need not adhere to a fixed schema\cite{cattell2011scalable}. This is well suited for modelling complex data structures. Many document stores use the JSON-fileformat (or some derived form). In contrast to columnar stores, the values in documents are not opaque to the system and can thus be queried and indexed\cite{hecht2011nosql}.
\item Graph databases: as the name suggests, these originate from graph theory and use graphs as their data model. They are especially useful to manage highly interconnected data coming from sources such as social networks or location based services, replacing costly operations like recursive joins with efficient graph traversals\cite{hecht2011nosql}.
\end{itemize}
The term NewSQL data stores is being used to classify a set of solutions that aim to combine the scalability, distribution and fault tolerance of NoSQL stores with the relational data model. Though they all use the relational model and supply SQL-querying capacities, NewSQL stores vary greatly under the hood, depending on the architecture they are built on\cite{grolinger2013data}. This paper proceeds first by elaborating on the methodology used to select and compare the chosen databases and then by reviewing each of the six datastores in detail.


\section{Methodology}

Because of the enormous choice of NoSQL and NewSQL datastores, an exhaustive study wasn't feasible. This survey reviews the most popular datastores in a few relevant cate\-gories, namely document stores, wide columnar stores and NewSQL stores. Key-value stores and graph databases were not taken into consideration, as their respective data models are not suited for the application at hand. A selection was then made based on similar criteria as in \cite{grolinger2013data}, following the ranking of DB-Engine Ranking \cite{db_engine_rank} as an indicator of popularity. 
\begin{landscape}
\includepdf[angle=90]{NoSQL_table.pdf}
\end{landscape}
This ranking tries to measure popularity based on a few parameters, such as the number of mentions on Web sites, general interest according to Google Trends, frequency of technical discussions on the Web, number of job listings, and number of professional profiles in which the systems are mentioned. The resulting selection consists of the document stores MongoDB and CouchBase Server, wide columnar stores Cassandra and HBase and NewSQL database VoltDB. There already exists an extension of the DNA sequencing pipeline used in the ExaScience Life Lab that allows for using MongoDB databases as input and/or output targets, making MongoDB even more relevant\cite{elprep_mongo}. Lastly, NewSQL query engine Cloudera Impala was also taken into consideration because of explicit interest from researchers in the aforementioned lab.
These 6 systems were then compared on characteristics relevant to high performance computing, such as indexing mechanisms, client interfaces to the data, distribution strategy, concurrency control and consistency models.

\section{Document stores}

\subsection{MongoDB}

MongoDB stores data in BSON (binary JSON) documents. It has powerful indexing support, with the ability to define secondary indexes of a wide array of types on all attributes, much like in the traditional relational model. These are implemented using B-trees \cite{mongodb_indexes}. To aid with denormalization, the documents can contain embedded documents and arrays, thus obviating the need for joins in the query language.
\\The document size is limited to 16 MB, to help ensure that a single document cannot take up excessive amounts of RAM or bandwidth. To store and retrieve larger files, the built-in tool GridFS (which is not an actual file system) can split up files in smaller chunks and store these chunks as separate documents\cite{mongodb_gridfs}.
\\MongoDB offers API's in many languages and the functionality to define the equivalent of SQL WHERE-clauses as javascript expressions. These are then translated to MongoDB's proprietary internal querying language \cite{grolinger2013data}. The MongoDB query optimizer processes queries and chooses the most efficient plan for a query given the available indexes. These plans are cached if there are multiple viable options and can be reevaluated as the data evolves \cite{mongodb_query_plans}.
\\The consistency of MongoDB is configurable. Strong consistency can be attained in two ways: setting the connection to read-only from the master node (which has the most up-to-date version of the data), or forcing a write to succeed only after all replicas have acknowledged it. The former degrades the scaling ability of read requests, the latter the latency of write requests \cite{grolinger2013data}.
\\Data is replicated asynchronously using range-partitioning: nodes are responsible for ranges of keys. This speeds up range queries, but can create hotspots and load-balancing issues. To route updates to the right replicas, MongoDB operates in a master-slave setting.\\
For concurrency control, MongoDB offers single-document atomicity and implements reader-writer locks. Having to lock on writes severely impacts performance in write-intensive scenarios.
\\\\As a wrap-up, MongoDB stores BSON-files accessible through many API's with flexible querying and indexing techniques, but its concurrency and consistency model have some drawbacks.

\subsection{CouchBase Server}

CouchBase, result of the merger of CouchDB and Membase, stores data in JSON documents. It uses the memcached protocol for distributed caching and is intended for highly interactive applications with low-latency requirements \cite{grolinger2013data}\cite{couchbase_about}.
\\The JSON documents can be nested and are queryable through a SQL-like language, N1QL (note: the most recent version at the time of writing, released in March 2014, is still a developer preview)\cite{couchbase_n1ql}. Primary and secondary indexes can be defined on the data and are implemented using B-trees\cite{couchbase_index}.
\\Within one cluster, transactions are strongly consistent, but between multiple clusters only eventually consistent.
\\Couchbase lets clients choose between optimistic (using compare-and-swap) and pessimistic (using 'finegrained locking') concurrency control.
\\\\With its flexible data modelling, caching and concurrency control, Couchbase is a good fit for applications requiring fast and intensive interaction between client and data.


\section{Columnar stores}
\subsection{Cassandra}

Apache Cassandra, originally developped at Facebook but later open-sourced, combines the data-model of Google's BigTable system with the architecture and distribution strategy of Amazon's DynamoDB. It is intended for flexible, highly-available storage of very large datasets, running on cheap commodity hardware and offering high write throughput while not sacrificing read efficiency \cite{lakshman2010cassandra}.
\\
Since its inception Cassandra has however diverged slightly from the BigTable data model. It now provides tables and composite columns -much like in a conventional schema, and comes with its own query language, CQL\cite{cassandra_then&now}. CQL resembles SQL in many ways, albeit with some restrictions. For instance, it doesn't feature the JOIN clause \cite{cassandra_cql}. It strongly encourages physically collocating data that will be queried together, and supports denormalization with features such as collection types.\\
Cassandra has primary and secondary indexing mechanisms and implements these - like BigTable \cite{chang2008bigtable}, using log-structured merge trees (or LSM trees). These allow for deferring updates and flushing them to disk in batches as soon as enough have accumulated, thus reducing disk I/O. This greatly benefits write throughput \cite{o1996log}\cite{sears2012blsm}\cite{lakshman2010cassandra}. Cassandra, like BigTable, also offers Bloom filters\cite{mullin1983second}. These are an efficient probabilistic mechanism to predict whether an item is in a set (in this case, whether a key is in a table) and can thus significantly reduce unnecessary table scans\cite{lakshman2010cassandra}.\\
In order to scale linearly in the number of nodes to very large datasets, Cassandra operates in a fully masterless fashion. In terms of the CAP-theorem, it focusses on availability and partition-tolerance, rather than immediate consistency (though the user has control over the level of consistency, as will be explained)\cite{brewer2000towards}. High availability and partition tolerance are achieved through asynchronous replication of rows over several nodes in the cluster, using consistent hashing and virtual nodes to handle high churn and incremental addition of nodes \cite{decandia2007dynamo} \cite{lakshman2010cassandra} \cite{cassandra_then&now}. The amount of replicas can be chosen by the client. Furthermore, Cassandra provides cross-datacenter replication to cope with entire datacenter failures.
\\On reads and writes, the client can specify the desired quorum, that is the number of replicas that acknowledge the operation. Although Cassandra was built with eventual consistency in mind, strong consistency can be obtained by choosing the quorum larger than the number of replicas \cite{grolinger2013data}.\\
In terms of concurrency control, Cassandra supports atomicity for single-row operations and serializable \textit{lightweight transactions}, essentially a compare-and-set functionality for larger operations \cite{cassandra_lightweight_trans}.\\
Being an open-source project, Cassandra is freely available, but there is an enterprise version with extra features such as integration with the data processing engine Apache Spark and the distributed search platform Apache Solr, for complex analytical and search tasks \cite{cassandra_solr} \cite{cassandra_spark} \cite{zaharia2010spark}\cite{apache_solr}.\\\\
In conclusion, Cassandra offers flexible data-modeling with decent querying and indexing support through its CQL-interface and scales incrementally to vast datasets, thanks to its extensive replication and failure-handling features.

\subsection{HBase}

Apache HBase is an open source datastore using Google BigTable's datamodel, but running on top of the Hadoop Distributed File System (HDFS) instead of the Google File System (GFS).
\\Since its launch, HBase has adopted several secondary indexing mechanisms. For indexing, HBase also uses LSM trees\cite{borthakur2011apache}\cite{sears2012blsm}, and it also provides Bloom filters\cite{hbase_schema}. HBase comes with a Java API but without an SQL-like advanced querying language, though a workaround via Apache Hive, another data warehousing and analytics project\cite{apache_hive}, and its query language HiveQL is possible. Because of its HDFS underpinnings it can easily function as both input and output for MapReduce jobs.
\\HBase partitions data in ranges like BigTable and replicates updates in master-slave or multi-master fashion. Read-requests are not distributed however, as rows are only serviced by one server. The replicas are intended solely for failure recovery.
\\The strong points of HBase are its strong consistency, which is rare among NoSQL-stores, and its concurrency model: ACID-compliant single row transactions and optimistic multi-version concurrency control for wider scope operations\cite{hbase_acid}\cite{grolinger2013data}\cite{borthakur2011apache}.\\\\
In conclusion, HBase allows users to flexibly model data and is especially useful when there is already a HDFS dataset and when MapReduce compatibility is a priority. It scales well to very large datasets and features excellent concurrency control and strong consistency.

\section{NewSQL}
\subsection{VoltDB}

VoltDB is a relational in-memory distributed database, aiming to couple the guarantees of classical SQL-stores with the scaling of NoSQL systems while performing at very high speeds\cite{stonebraker2013voltdb}.
\\VoltDB stores data in the traditional relational model, but replicated and partitioned (using consistent hashing) over several nodes\cite{grolinger2013data}. The data is queryable through a (growing) subset of SQL-92 \cite{voltdb2010voltdb}. Queries are preferably defined as stored procedures written in Java, in which the SQL statements are embedded. VoltDB supports primary and secondary indexing, and gives the user the choice between hash- and tree-indexes\cite{voltdb_indexes}. VoltDB plans and optimizes queries in stored procedures optimized at compile time\cite{voltdb_query_plans}.\\
Relying entirely on DRAM makes it expensive to scale to petabyte scale datavolumes, but VoltDB can export data to other, more suited DMBSs such as columnar NoSQL stores.
\\In terms of concurrency control, VoltDB supports full ACID-transactions which execute simultaneously on all replicas. Main memory is divided into chunks and these are statically assigned to individual, single-threaded cores. A global controller serializes all multi-node transactions to a sequence of single-node transactions and inserts those into the transactions queues of the respective nodes. In this way, VoltDB obviates the need for locking and latching techniques.
\\\\
In short, VoltDB provides relational in memory storage for relatively large datasets, with very fast SQL-query capacities and ACID transactions. It is thus more suited for compute intensive applications that don't work on exorbitantly large data volumes but require very low latency.

\subsection{Cloudera Impala}

Cloudera Impala is a distributed SQL-engine running on top of the Hadoop stack, either on HDFS or HBase \cite{cloudera_impala}. It is intended specifically for analytical use, focussing on delivering real-time querying capacities and not on high-throughput on writes.
\\Data is accessible through a subset of SQL-92.
\\Impala's architecture is almost perfectly symmetrically distributed: all nodes execute the same \texttt{impalad}-process, which is responsible for the main database functionality. Queries can be sent to any node which will then function as coordinator for the specific query. There are however two processes that run on only one (not necessarily the same) node in the cluster, performing bookkeeping tasks and relaying metadata changes through the cluster\cite{impala_components}.
\\Contrary to the options discussed before, Impala doesn't partition rows by default. It does give the user the possibility to partition data should the data size call for it\cite{impala_partitioning}.
\\Impala has a highly efficient I/O-layer keeping disk- and CPU-utilization high at all times, resulting in considerably faster performance than other SQL-on-Hadoop solutions such as Apache Hive \cite{floratou2014sql}. However, this comes with the drawback that the working set of a query has to fit in the aggregate physical memory of the cluster it runs on. This puts some restrictions on the size of datasets processable with Impala, not fully using the scaling capacity of the underlying Hadoop layer.
\\Because of its analytical purposes, Impala doesn't have extensive concurrency control features, relying instead on the underlying storage layer. With HBase's excellent concurrency control features, this doesn't necessarily pose a problem.
 
\section{Conclusions}

In recent years, progress in microbiology and bioinformatics has driven down the cost of DNA-sequencing. As DNA of more and more organisms is being sequenced, the question arises to make this process ever more efficient and to store the generated data in a scalable, performant and accessible way. This study has focused on NoSQL and NewSQL datastores to be used in both the sequencing pipeline as in the storage and analysis of the results. These solutions offer elastic scalability, flexible data modelling, good behavior in a distributed setting and resilience to failures.\\
Specifically, this paper has reviewed 6 different datastores, 2 of the most popular ones in three categories, and compared them on a selection of properties relevant to HPC applications. Columnar stores like Apache Cassandra and HBase are well suited to the task of storing extremely large datasets in a reliable and performant way, and could handle the storage of already sequenced genomes. MongoDB sets itself apart with its extensive API, flexible data modeling, querying and indexing features. It doesn't have quite the write-handling capacities of the columnar stores, but is nevertheless a strong candidate for storing very large datasets such as the sequenced genomes. VoltDB and CouchBase Server offer low latency on reads and writes, albeit (especially VoltDB) on smaller datasets, but would thus be very useful for storing rapidly changing intermediate data in the sequencing pipeline. Lastly, Cloudera Impala lends itself well to situations where fast read, but not write queries on large datasets are required.

\bibliography{biblio}{}
\bibliographystyle{plain}
\end{document}
